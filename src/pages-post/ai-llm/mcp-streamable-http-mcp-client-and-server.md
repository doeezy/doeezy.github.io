---
title: "[MCP] Streamable HTTP ê¸°ë°˜ MCP Client / Server"
menu: ai-llm
date: 2026-02-09
tags:
  - MCP Client
  - MCP Server
  - LLM
  - AI Agent
---

# ğŸ› ï¸ ìŠ¤í™

### 1. MCP SDK ë¼ì´ë¸ŒëŸ¬ë¦¬ -> MCP í”„ë¡œí† ì½œ ê·œê²© ë¼ì´ë¸ŒëŸ¬ë¦¬

- Client â†” Server ê°„ í‘œì¤€ ë©”ì‹œì§€ ê·œê²©(JSON-RPC ê¸°ë°˜) ì²˜ë¦¬
- initialize, list_tools, call_tool ê°™ì€ MCP í‘œì¤€ API ì œê³µ

### 2. Streamable HTTP ê¸°ë°˜ í†µì‹ 

### 3. Python

### 4. OpenAI API(LLM)


# ë¡œì»¬ MCP Server êµ¬í˜„

1. **MCP ServerëŠ” íˆ´ ì‹¤í–‰ê¸°**

    â†’ ë“±ë¡ëœ íˆ´ì„ ì‹¤í–‰í•´ì„œ **ê²°ê³¼ë§Œ ë°˜í™˜**í•˜ëŠ” ì—­í• 


2. **tools/listëŠ” ì§ì ‘ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ë¨**

    â†’ MCP SDK(FastMCP)ê°€ ì„œë²„ ê¸°ë™ ì‹œ ë“±ë¡ëœ íˆ´ë“¤ì„ ìŠ¤ìº”í•¨.

    list_tools() ìš”ì²­(=tools/list)ì— ëŒ€í•´ ìë™ìœ¼ë¡œ íˆ´ ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜í•´ì¤Œ.

    ì„œë²„ì—ëŠ” @mcp.tool()ë¡œ í•¨ìˆ˜ë§Œ ë“±ë¡í•˜ë©´ ë¨.


3. **íˆ´ ë©”íƒ€ë°ì´í„° ìë™ ìƒì„±**

    â†’ MCP SDK ì‹¤í–‰ì‹œ ë‚´ë¶€ íë¦„

    - @mcp.tool()ë¡œ ë“±ë¡ëœ í•¨ìˆ˜ ìŠ¤ìº”
    - í•¨ìˆ˜ ì´ë¦„ â†’ tool.name
    - í•¨ìˆ˜ docstring â†’ tool.description
    - í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ + íƒ€ì… íŒíŠ¸ â†’ tool.inputSchema (Json Schema)

4. **docstring(=description)ì€ LLMì„ ìœ„í•œ ê°€ì´ë“œ**

    â†’ LLMì´ íˆ´ì„ ê³ ë¥¼ ë•Œ ë³¼ ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” ëŒ€ë¶€ë¶„

    - tool name
    - tool description
    - input schema

    ì´ê¸° ë•Œë¬¸ì— descriptionì´ ë¹ˆì•½í•˜ë©´ íˆ´ì„ ì•ˆ ì“°ê±°ë‚˜ ì˜ëª»ëœ íˆ´ì„ ê³ ë¥´ëŠ” ê²½ìš°ê°€ ìƒê¹€

<br/>

```python
from mcp.server.fastmcp import FastMCP

# Initialize FastMCP server
mcp = FastMCP("Local MCP PoC", json_response=True, stateless_http=True)

@mcp.tool()
def add(a: int, b: int) -> int:
    """
    Add two numbers and return the exact sum.
    Use this tool when the user asks for arithmetic calculation
    or when an exact numeric result is required.
    Do not use this for estimation or explanation.
    """  # toolì˜ descriptionìœ¼ë¡œ ìƒì„±ë¨

    # ë‘ ìˆ«ìë¥¼ ë”í•´ ì •í™•í•œ í•©ê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    # ì‚¬ìš©ìê°€ ì‚°ìˆ  ê³„ì‚°ì„ ìš”ì²­í–ˆê±°ë‚˜
    # ì •í™•í•œ ìˆ«ì ê²°ê³¼ê°€ í•„ìš”í•œ ê²½ìš°ì— ì´ íˆ´ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    # ì¶”ì •ì´ë‚˜ ì„¤ëª… ìš©ë„ë¡œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

    return {"a": a, "b": b, "sum": a + b}

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """
    Multiply two numbers and return the exact product.
    Use this tool when the user asks for arithmetic calculation
    or when an exact numeric result is required.
    Do not use this for estimation or explanation.
    """
    # ë‘ ìˆ«ìë¥¼ ê³±í•˜ì—¬ ì •í™•í•œ ê³±ì…ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    # ì‚¬ìš©ìê°€ ì‚°ìˆ  ê³„ì‚°ì„ ìš”ì²­í–ˆê±°ë‚˜
    # ì •í™•í•œ ìˆ«ì ê²°ê³¼ê°€ í•„ìš”í•œ ê²½ìš°ì— ì´ íˆ´ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    # ì¶”ì •ì´ë‚˜ ì„¤ëª… ìš©ë„ë¡œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

    return {"a": a, "b": b, "sum": a * b}

USERS = {
    "ë„í¬ì •": {
        "role": "member",
        "role_name": "í”„ë¡œ",
        "department": "IA íŒ€",
        "email": "dohxxzun@mobigen.com",
        "phone": "010-6210-xxxx",
    },
    "ì´ë™ì£¼": {
        "role": "team-leader",
        "role_name": "íŒ€ì¥",
        "department": "IA íŒ€",
        "email": "ainory@mobigen.com",
        "phone": "010-8792-xxxx",
    },
}

@mcp.tool()
def get_user(username: str) -> dict:
    """
    Retrieve user information by username and return the stored user data.
    Use this tool when the user asks about a specific user's details,
    such as role, profile, or any stored attributes.
    This tool is the authoritative source of user information.
    Do not guess or fabricate user data without calling this tool.
    """
    # ì‚¬ìš©ì ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ëœ ì‚¬ìš©ì ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    # íŠ¹ì • ì‚¬ìš©ìì˜ ì—­í• , í”„ë¡œí•„ ë“± ì„¸ë¶€ ì •ë³´ë¥¼ ë¬¼ì–´ë³¼ ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    # ì´ íˆ´ì€ ì‚¬ìš©ì ì •ë³´ì— ëŒ€í•œ ì‹ ë¢° ê°€ëŠ¥í•œ ë‹¨ì¼ ì¶œì²˜ì…ë‹ˆë‹¤.
    # ì´ íˆ´ì„ í˜¸ì¶œí•˜ì§€ ì•Šê³  ì‚¬ìš©ì ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

    user = USERS.get(username)
    if not user:
        return {
            "ok": False,
            "error": {
                "code": "NOT_FOUND",
                "message": "User not found",
                "username": username,
            },
        }

    return {"ok": True, "user": user}

if __name__ == "__main__":
    print("Starting MCP Server on <http://localhost:8000/mcp>")
    # ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ëŠ” <http://localhost:8000/mcpë¡œ> ëœ¸
    mcp.run(transport="streamable-http")
```


# ë¡œì»¬ MCP Client êµ¬í˜„

### ğŸ“Œ

ì´ë²ˆ PoCì˜ LLMì€ OpenAIì˜ Chat Completions API í‘œì¤€ì„ ë”°ëìŒ

í•´ë‹¹ ê·œê²©ì€ ê³µì‹ë¬¸ì„œì—ì„œ í™•ì¸ ê°€ëŠ¥ ğŸ‘‡ğŸ¾

https://platform.openai.com/docs/api-reference/chat/create


### ë™ì‘ íë¦„

1. session.initialize(): MCP í•¸ë“œì…°ì´í¬ë¡œ ì„¸ì…˜ ì‹œì‘
2. session.list_tools(): ì„œë²„ê°€ ì œê³µí•˜ëŠ” tool matadata ìˆ˜ì§‘(íˆ´ ëª©ë¡)
3. tool metadataë¥¼ LLM tool schema(OpenAI API ê·œê²©)ë¡œ ë³€í™˜
4. ì‚¬ìš©ì ì§ˆë¬¸ + toolsë¥¼ LLMì— ì „ë‹¬
5. LLM ì‘ë‹µì—ì„œ tool_calls íŒŒì‹±
6. session.call_tool(name, args)ë¡œ ì„œë²„ tool ì‹¤í–‰
7. tool ê²°ê³¼ë¥¼ role=â€toolâ€ ë©”ì‹œì§€ë¡œ LLMì— ì¬íˆ¬ì…
8. ìµœì¢… contentê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ë°˜ë³µ

```python
import asyncio
import json
import os
from typing import Any, Dict, List, Optional

import httpx
from dotenv import load_dotenv

from mcp import ClientSession
from mcp.client.streamable_http import streamable_http_client
from mcp import types as mcp_types

def mcp_tools_to_openai_tools(mcp_tools: List[mcp_types.Tool]) -> List[Dict[str, Any]]:
    """
    MCP tool -> OpenAI tools(function calling) í˜•íƒœë¡œ ë³€í™˜.
    MCP Toolì˜ inputSchemaë¥¼ OpenAI function.parametersë¡œ ê·¸ëŒ€ë¡œ ì”€.
    """
    out: List[Dict[str, Any]] = []
    for t in mcp_tools:
        out.append(
            {
                "type": "function",
                "function": {
                    "name": t.name,
                    "description": t.description or "",
                    "parameters": t.inputSchema or {"type": "object", "properties": {}},
                },
            }
        )
    return out

async def llm_chat(
    base_url: str,
    api_key: str,
    model: str,
    messages: List[Dict[str, Any]],
    tools: Optional[List[Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    payload: Dict[str, Any] = {"model": model, "messages": messages}
    if tools is not None:
        payload["tools"] = tools
        payload["tool_choice"] = "auto"

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    async with httpx.AsyncClient(timeout=90.0) as client:
        r = await client.post(
            f"{base_url.rstrip('/')}/chat/completions", headers=headers, json=payload
        )
        r.raise_for_status()
        return r.json()

def get_choice_message(resp: Dict[str, Any]) -> Dict[str, Any]:
    # llm(openai api ê¸°ì¤€)ì˜ ì‘ë‹µì—ì„œ ì„ íƒëœ ë©”ì‹œì§€ë¥¼ ì¶”ì¶œ
    choices = resp.get("choices", [])
    if not choices:
        return {"role": "assistant", "content": "LLM returned no choices."}
    return choices[0].get("message", {"role": "assistant", "content": ""})

def as_text_from_calltool_result(result: mcp_types.CallToolResult) -> str:
    """
    MCP CallToolResultë¥¼ LLMì— ë„£ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ ë³€í™˜.
    json_response=Trueë©´ structuredContentê°€ ì±„ì›Œì§ˆ ê°€ëŠ¥ì„±ì´ í¼.
    """
    if getattr(result, "structuredContent", None):
        return json.dumps(result.structuredContent, ensure_ascii=False)

    # fallback: content blockë“¤ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
    parts = []
    for c in result.content or []:
        if isinstance(c, mcp_types.TextContent):
            parts.append(c.text)
        else:
            parts.append(str(c))
    return "\n".join(parts).strip() or "{}"

async def run_agent(user_text: str) -> str:
    load_dotenv()

    mcp_url = "http://127.0.0.1:8000/mcp"

    llm_base = os.environ["LLM_BASE_URL"]
    llm_key = os.environ["LLM_API_KEY"]
    llm_model = os.environ.get("LLM_MODEL", "gpt-4.1-mini")

    system_prompt = (
        "You are an agent. Use tools when helpful. "
        "If you call a tool, use the tool result to produce the final answer."
    )

    # MCP ì„œë²„ ì—°ê²° (Streamable HTTP)
    async with streamable_http_client(mcp_url) as (read_stream, write_stream, _meta):
        async with ClientSession(read_stream, write_stream) as session:
            await session.initialize()  # MCP í•¸ë“œì…°ì´í¬

            # MCP SDKì—ì„œ ì œê³µí•˜ëŠ” ClientSessionì˜ ë‚´ì¥ í•¨ìˆ˜
            # MCP í”„ë¡œí† ì½œì˜ tools/list ìš”ì²­
            tools_resp = await session.list_tools()
            # MCP tool -> OpenAI tools(function calling) í˜•íƒœë¡œ ë³€í™˜.
            openai_tools = mcp_tools_to_openai_tools(tools_resp.tools)

            messages: List[Dict[str, Any]] = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_text},
            ]

            # LLM tool-calling ë£¨í”„
            for _ in range(8):
                resp = await llm_chat(
                    llm_base, llm_key, llm_model, messages, tools=openai_tools
                )
                assistant_msg = get_choice_message(resp)

                # llm(openai api ê¸°ì¤€)ì˜ ì‘ë‹µì—ì„œ tool_callsë¥¼ ì¶”ì¶œ
                tool_calls = assistant_msg.get("tool_calls") or []
                messages.append(assistant_msg)

                # tool_calls ì—†ìœ¼ë©´ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì¢…ë£Œ
                if not tool_calls:
                    return assistant_msg.get("content", "") or ""

                print(f"============ [DEBUG] ì„ íƒëœ Tools: {tool_calls} ============")

                # tool ì‹¤í–‰
                for tc in tool_calls:
                    fn = tc.get("function") or {}
                    name = fn.get("name")
                    raw_args = fn.get("arguments") or "{}"

                    try:
                        args = (
                            json.loads(raw_args)
                            if isinstance(raw_args, str)
                            else (raw_args or {})
                        )
                    except Exception:
                        args = {}

                    # MCP tool call
                    result = await session.call_tool(name, arguments=args)
                    tool_text = as_text_from_calltool_result(result)
                    print(f"============ [DEBUG] Tool Result: {tool_text} ============")

                    # OpenAI tool message í˜•íƒœë¡œ LLMì— ê²°ê³¼ ì „ë‹¬
                    messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tc.get("id"),
                            "content": tool_text,
                        }
                    )

            return "Tool loop limit reached."

async def main():
    print("MCP Client Agent (type 'exit' to quit)")
    while True:
        q = input("ì§ˆë¬¸: ").strip()
        if q.lower() in ("exit", "quit"):
            break
        ans = await run_agent(q)
        print(f"ë‹µë³€: {ans}")

if __name__ == "__main__":
    asyncio.run(main())

```