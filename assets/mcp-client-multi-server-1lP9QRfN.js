const n=`---
title: "[MCP] Mcp Client ‚Üî Î©ÄÌã∞ Mcp Server Ïó∞Îèô"
menu: ai-llm
date: 2026-02-09
tags:
  - MCP Server
  - MCP Client
  - LLM
  - AI Agent
---

MCP ClientÎäî Ïó¨Îü¨ MCP ÏÑúÎ≤ÑÏùò toolÏùÑ ÌÜµÌï©ÌïòÏó¨ LLMÏóê ÎÖ∏Ï∂úÌïòÍ≥†, LLMÏù¥ ÏÑ†ÌÉùÌïú toolÏùÑ Ïã§Ï†ú MCP ÏÑúÎ≤ÑÎ°ú ÎùºÏö∞ÌåÖ¬∑Ïã§ÌñâÌïòÎäî Ïò§ÏºÄÏä§Ìä∏Î†àÏù¥ÌÑ∞ Ïó≠Ìï†ÏùÑ ÏàòÌñâ

## üöÄ¬†ÌîåÎ°úÏö∞

1. Ïó¨Îü¨ MCP ÏÑúÎ≤ÑÏóê Ïó∞Í≤∞ÌïòÏó¨ ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî
    - Ïù¥Î≤à PoCÏóêÏÑúÎäî Î°úÏª¨ MCP ÏÑúÎ≤ÑÏôÄ SerpApi MCP ÏÑúÎ≤Ñ ÏÇ¨Ïö©
2. Í∞Å MCP ÏÑúÎ≤ÑÎ°úÎ∂ÄÌÑ∞ tool Î™©Î°ù Ï°∞Ìöå Î∞è ÏÑúÎ≤ÑÍ∞Ä Ï†úÍ≥µÌïòÎäî tool metadata ÏàòÏßë
3. Í∞Å MCP ÏÑúÎ≤ÑÏùò tool Î™©Î°ùÏùÑ Î≥ëÌï©
    - ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§(prefix)Î°ú tool name Ï§ëÎ≥µ Î∞©ÏßÄ
4. ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏Í≥º Ìï®Íªò Î≥ëÌï©Îêú tool Î™©Î°ùÏùÑ LLMÏóê Ï†ÑÎã¨
5. LLMÏù¥ toolÏùÑ ÏÑ†ÌÉùÌïú Í≤ΩÏö∞(tool_calls) Ìï¥Îãπ toolÏùÑ ÎùºÏö∞ÌåÖÌïòÏó¨ Ïã§ÌñâÌï®
    - tool name(prefix)ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÎåÄÏÉÅ MCP ÏÑúÎ≤Ñ ÏÑ∏ÏÖò ÏÑ†ÌÉù
    - Ïã§Ï†ú MCP tool nameÍ≥º argumentsÎ°ú call_tool() ÏàòÌñâ

\`\`\`python
# CASE 2: Î©ÄÌã∞ Mcp ÏÇ¨Ïö©Ìï† Í≤ΩÏö∞
# OpenAI ÏöîÏ≤≠ÏùÑ ÏúÑÌïú Tools + ÎùºÏö∞ÌåÖÎßµ ÏÉùÏÑ±
def build_openai_tools_and_routes(
    server_key: str,
    mcp_tools: List[mcp_types.Tool],
) -> Tuple[List[Dict[str, Any]], Dict[str, Dict[str, str]]]:
    """
    server_keyÎ•º prefixÎ°ú Î∂ôÏó¨ÏÑú tool name Ï∂©Îèå Î∞©ÏßÄ:
      - OpenAI tool name: "{server_key}.{tool_name}"
      - route_map: namespaced_name -> {"server_key": ..., "tool_name": ...}
    """
    tools_out: List[Dict[str, Any]] = []
    route_map: Dict[str, Dict[str, str]] = {}

    for t in mcp_tools:
        namespaced = f"{server_key}-{t.name}"
        tools_out.append(
            {
                "type": "function",
                "function": {
                    "name": namespaced,
                    "description": t.description or "",
                    "parameters": t.inputSchema or {"type": "object", "properties": {}},
                },
            }
        )
        route_map[namespaced] = {"server_key": server_key, "tool_name": t.name}

    return tools_out, route_map
    
 async def run_agent(user_text: str) -> str:
    load_dotenv()

    llm_base = os.environ["LLM_BASE_URL"]
    llm_key = os.environ["LLM_API_KEY"]
    llm_model = os.environ.get("LLM_MODEL", "gpt-4.1-mini")
    serapi_key = os.environ.get("SERPAPI_KEY", "")
    _base_url = os.environ.get("SERPAPI_MCP_URL", "")
    serpapi_mcp_url = (
        f"{_base_url}/{serapi_key}/mcp" if serapi_key and _base_url else _base_url
    )

    MCP_SERVERS: Dict[str, str] = {
        "local": "http://127.0.0.1:8282/mcp",
        "serpapi": serpapi_mcp_url,
    }

    system_prompt = (
        "You are an agent. Use tools when helpful. "
        "If you call a tool, use the tool result to produce the final answer."
    )

    async with AsyncExitStack() as stack:
        sessions: Dict[str, ClientSession] = {}

        # Í∞Å Mcp ÏÑúÎ≤Ñ initialize + tools/list Ìò∏Ï∂ú
        openai_tools: List[Dict[str, Any]] = []
        route_map: Dict[str, Dict[str, str]] = {}

        for key, url in MCP_SERVERS.items():
            if not url:
                print(f"[WARN] Skipping {key} because URL is empty")
                continue

            try:
                # streamable_http_client Ïª®ÌÖçÏä§Ìä∏ ÏßÑÏûÖ
                ctx = streamable_http_client(url)
                read_stream, write_stream, _meta = await stack.enter_async_context(ctx)

                # ClientSession Ïª®ÌÖçÏä§Ìä∏ ÏßÑÏûÖ
                session = ClientSession(read_stream, write_stream)
                await stack.enter_async_context(session)
                await session.initialize()

                sessions[key] = session

                tools_resp = await session.list_tools()
                # MCP tool -> OpenAI tools(function calling) ÌòïÌÉúÎ°ú Î≥ÄÌôò.
                tools_out, routes_out = build_openai_tools_and_routes(
                    key, tools_resp.tools
                )
                openai_tools.extend(tools_out)
                route_map.update(routes_out)
            except Exception as e:
                print(f"[ERROR] Failed to connect to {key} at {url}: {e}")
                continue

        messages: List[Dict[str, Any]] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_text},
        ]

        # LLM tool-calling Î£®ÌîÑ
        MAX_TOOL_TURNS = 6
        for _ in range(MAX_TOOL_TURNS):
            resp = await llm_chat(
                llm_base, llm_key, llm_model, messages, tools=openai_tools
            )
            assistant_msg = get_choice_message(resp)

            tool_calls = assistant_msg.get("tool_calls") or []
            messages.append(assistant_msg)

            if not tool_calls:
                return assistant_msg.get("content", "") or ""

            print(f"============ [DEBUG] ÏÑ†ÌÉùÎêú Tools: {tool_calls} ============")

            # tool Ïã§Ìñâ(ÎùºÏö∞ÌåÖ)
            for tc in tool_calls:
                fn = tc.get("function") or {}
                namespaced_name = fn.get("name")
                raw_args = fn.get("arguments") or "{}"

                if not namespaced_name or namespaced_name not in route_map:
                    messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tc.get("id"),
                            "content": json.dumps(
                                {
                                    "ok": False,
                                    "error": {
                                        "code": "UNKNOWN_TOOL",
                                        "name": namespaced_name,
                                    },
                                },
                                ensure_ascii=False,
                            ),
                        }
                    )
                    continue

                try:
                    args = (
                        json.loads(raw_args)
                        if isinstance(raw_args, str)
                        else (raw_args or {})
                    )
                except Exception:
                    args = {}

                print(f"============ [DEBUG] args: {args} ============")

                route = route_map[namespaced_name]
                server_key = route["server_key"]
                real_name = route["tool_name"]

                session = sessions.get(server_key)
                if not session:
                    messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tc.get("id"),
                            "content": json.dumps(
                                {"ok": False, "error": "Session not available"}
                            ),
                        }
                    )
                    continue

                try:
                    result = await session.call_tool(real_name, arguments=args)
                    tool_text = as_text_from_calltool_result(result)
                except Exception as e:
                    tool_text = json.dumps({"ok": False, "error": str(e)})

                messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": tc.get("id"),
                        "content": tool_text,
                    }
                )

        return "Tool loop limit reached."
\`\`\``;export{n as default};
